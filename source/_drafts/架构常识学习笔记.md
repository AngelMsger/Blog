---
title: 架构常识学习笔记
date: 2021-03-18 11:24:21
tags:
- Distributed
categories:
- Database
thumbnail: "/images/banner/架构常识学习笔记.jpg"
typora-root-url: ../../source/
---

# 分布式原理

## CAP及其衍生理论

**CAP**原理即**C**onsistency(数据一致性), **A**valiability(可用性)和**P**artition tolerance(分区容错性)在分布式系统中只能同时满足两个目标. 由于分区容错性(P)是必须保留的, 所以往往在数据一致性(C)和可用性(A)间取舍.

由于网络分区是极端情况, 因此衍生出改进的模型**PACELC**, 即如果网络发生分区(P), 则在可用性(A)和数据一致性(C)间权衡, 否则(E, else)在时延(L, letency)和数据一致性(C)间权衡.

实践中目标的选择往往并不是非此即彼的, 此时需要引入**BASE**理论, 即**B**asically **A**vailable(基本可用), **S**oft state(软状态)和**E**ventually consistent(最终一致性). 该理论指导我们在实践中根据自身业务的特点, 以最终一致性代替强一致性从而尽可能保证服务基本可用. 所谓**基本可用**, 即是在可用性方面做出妥协, 如:

* 电商网站在双十一大促期间访问压力较大时, 会关闭排行榜等次要功能, 或直接返回静态的兜底数据, 从而保证交易流程的可用性, 即**服务降级**.
* 为错开双十一高峰, 会将预售商品支付时间延后, 即**流量削峰**.

**软状态**和**最终一致性**是指允许系统中的数据处于中间状态, 即牺牲事件窗口内的数据一致性, 但保证最终的一致性. 如:

* 目前主流数据库的高可用方案均为主从复制, 为了提高写入的成功率和降低响应时间, 主节点通常仅在一部分(通常为超过半数)从节点确认后就返回成功.

由于牺牲了短时间内的数据一致性, 因此可能引入新的问题. 比如在读写分离场景中:

* 提交了一条评论(写主节点), 但刷新后评论消失了(读从节点, 且目标从节点尚未同步到主节点的写入数据).
* 多次刷新页面结果不一致(读不同的从节点).

对于上述问题, 其后又产生了介于强一致性和最终一致性之间的读写一致性, 单调读和因果一致性等思路, 我在《数据密集型应用系统设计》一书的读后总结文章中讨论过相关问题, 这里不再展开了.

理解上述理论之后, 就可以回答前文中提到的一个问题, **Redis是否适合实现分布式锁?**

我们一般使用Redis的`SETNX`方法实现置锁和超时时间, 在Redis主节点挂掉的时候, 根据Sentinel机制, 此时从节点将上升为主节点继续提供服务, 如果此时锁没有同步到从节点, 而另一个线程也在获取锁资源, 就有可能出现两个线程同时取到锁的情况.

根据我们对CAP理论的理解, Redis的设计模型属于AP模型, 而分布式锁是一个CP场景, 因此用Redis不适合实现分布式锁. Redis作者也意识到这一问题, 因此提出了[RedLock](https://redis.io/topics/distlock)方案.

## 分布式系统的知识体系

分布式系统的结构与冯诺依曼结构的计算机类似, 同样包含控制器, 运算器, 存储器和输入输出, 关系如下:

1. **控制器**: 流量调度, 资源调度.
2. **运算器**: 分布式并行计算.
3. **存储器**: 分布式存储.
4. **输入输出**: RPC调用及消息队列.

其中, 存储和运算又是其中的重点.

### 存储

以亿级数据存储设计为例, 考虑如下问题:

* 如何设计可扩展的存储架构?
* 常见的分片方式有哪些, 哈希取模与一致性哈希如何实现?
* 流量高峰时期如何调整热点数据的存储策略?
* 强一致性和最终一致性的数据共识算法如何实现?

#### 横向扩容

当数据规模上升时, 为解决单台设备的局限性, 往往会把数据分布到多个存储节点上来实现水平扩展, 从而降低单个节点的读写压力. 这一过程也称为**分片**, 确定数据所属存储节点的规则即分片策略, 常见的分片策略由**哈希分片**(Hash Based)和**范围分片**(Range Based). 如MongoDB, PostgreSQL等数据库就支持范围分片策略(MongoDB虽然也提供"哈希分片", 但根据其[文档](https://docs.mongodb.com/manual/sharding/#hashed-sharding), "Hashed Sharding involves computing a hash of the shard key field’s value. Each chunk is then assigned a **range based** on the hashed shard key values.", 可以看出本质依然是范围分片).

两种策略各有优劣, 以哈希分片为例, 其能够将数据更均匀的分布在各个分片上, 并且实现简单, 但对顺序读写场景不友好, 并且扩展性很差. 如下图(左)所示, 以ID对分片数量取模作为哈希函数, 当分片数量变化时, 大量数据将由于哈希结果发生变化而在分片间迁移.

![一致性哈希](/images/%E6%9E%B6%E6%9E%84%E5%B8%B8%E8%AF%86%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E4%B8%80%E8%87%B4%E6%80%A7%E5%93%88%E5%B8%8C-6038905.png)

为应对上述问题, 同时保证数据的均匀分布和集群的扩展性, 现代数据库引入了一致性哈希算法. 如上图(右)所示, 数据和分片都映射在哈希环上, 在逻辑上数据根据其哈希结果落在环上的特定位置, 在物理上则由延环顺时针方向的下一个分片存储. 这样, 即使分片的数量增加或减少, 也只影响环上的相邻节点, 不会发生大规模数据迁移.

但上述方案存在一个缺陷, 即增加节点只能分摊一个相邻节点的压力, 而节点故障时, 该节点的压力也将全部转向下一个节点. 针对这一问题, 使用一致性哈希算法的数据库往往会引入**虚拟节点**(Virtual Node, VNode)概念, 用虚拟节点代替物理节点映射到哈希环上. 虚拟节点的数量远大于物理节点, 因此一个物理节点将负责多个虚拟节点的存储. 这就保证了当哈希环上增加或删除节点时, 将有多个物理节点参与响应, 达到新的均衡状态. 还可以根据物理节点的性能调整其承载虚拟节点的数量, 从而更加合理的分配资源.

一致性哈希算法依赖维护哈希环的元数据, 操作数据前需要先从哈希环元数据中找到虚拟节点及其对应的物理节点.

以Cassandra为例, 其分片策略即是上述的基于虚拟节点的一致性哈希算法, 可以在[这篇文档](https://cassandra.apache.org/doc/latest/architecture/dynamo.html#dataset-partitioning-consistent-hashing)中了解更加详细的内容. MongoDB的"哈希分片"方案由于是先计算哈希, 再按照哈希的范围决定分片, 并且一个节点负责多个范围(Chunk), 因此效果和基于虚拟节点的一致性哈希算法有类似之处.

范围分片的优势则在于可以根据业务指定和调整规则, 如动态分片. 但高灵活性也带来了高复杂性. 需要在外部维护高可用的元数据存储和代理服务. 得益于外围组件如Etcd的发展, 元数据存储的复杂度可以转嫁, 因此一个典型的结构如下:

![范围分片](/images/%E6%9E%B6%E6%9E%84%E5%B8%B8%E8%AF%86%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E8%8C%83%E5%9B%B4%E5%88%86%E7%89%87.png)

#### 数据一致性和高可用

为防止系统出现单点故障, 实现存储层的高可用, 数据往往需要被**复制**, 产生多个副本, 也即我们常常提到的主从模式. 当主节点发生故障时, 从节点将替代主节点继续对外提供服务. 这就回到了前文提及的的**数据一致性**问题, 解决方案根据场景和一致性强度不同又涉及2PC(Two-Phrase Commit, 二阶段提交), Paxos协议, Raft协议和Gossip协议等. 协议的细节设计内容较多, 我最近在研究Etcd, 之后在介绍Etcd的文章中再讨论这些共识算法.

#### 分布式事务

单体系统被拆分后, 一次写入操作可能涉及多个子系统, 这就带来了分布式事务的实现和性能问题. 典型的解决方案有2PC, 3PC, TCC和基于消息队列等. 但除了可靠消息投递外的方案由于落地代价很大, 除非强一致性业务场景(如支付), 在实践中很少使用.

##### 二阶段提交

2PC是数据库领域解决分布式事务的经典协议. 由**协调者**(Coordinator, 即事务管理器)和**参与者**(Participant, 即资源管理器)共同完成. 我们熟悉的分布式事务规范XA即定义了全局**事务管理器**(Transaction Manager)和局部**资源管理器**(Resource Manager)间的接口. 事务管理器进行事务协调, 资源管理器处理实际资源(如MySQL, Oracle等). Java平台上的事务规范**JTA**(Java Transaction API)就是对XA规范的实现. 下图展示了2PC处理事务的基本过程:

![2PC](/images/%E6%9E%B6%E6%9E%84%E5%B8%B8%E8%AF%86%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/2PC.png)

1. **准备**(Prepare)阶段: 事务管理器通知所有资源管理器做好事务提交准备. 此时资源管理器需要记录Undo和Redo日志(我在[MySQL存储结构相关的文章](https://blog.angelmsger.com/InnoDB存储结构/)中讨论过这两种日志的作用), 并返回OK, 否则如果出错, 如取锁失败, 需要返回Error. 需要注意, **一旦返回OK, 资源管理器需要保证在任何情况下该事务都能被提交**. 因此, 将准备阶段理解为本地事务中的开启但未提交的状态是错误的, 因为在这种情况下, 即使已经向资源管理器回复了OK, 如果对应数据库实例故障重启, 则重启后当前事务会被回滚, 就不在满足之前的约束条件了.
2. **提交**(Commit)/**中断**(Abort)阶段:  当所有资源管理器返回OK后, 事务管理器将向所有资源管理器发出提交事务的指令. 或任一资源管理器返回Error后, 事务管理器将向所有资源管理器发出回滚事务的指令.

上述过程只是2PC的基本实现, 实际上在任何一个步骤中, 都有可能受网络等因素影响而产生失败或延时. 这些情况发生时保证数据一致性和降低服务的不可用时间, 又将引入**终止协议**(Termination Protocol)等逻辑, 进一步提升了系统的复杂性. 此外, 由于准备过程中需要对多个实例的资源进行锁定, 因此就产生了性能问题和死锁的风险. 正是由于这些原因, 2PC虽然是一些数据库提供的分布式事务解决方案, 但在实践中的应用却很有限.

##### 可靠消息投递

与前文的理论想对应, 基于消息队列的方案, 实际上是放弃强一致性而选择最终一致性的结果. 消息队列解决了同步执行的阻塞问题, 实现了业务解耦和流量削峰, 在开源界也有这RabbitMQ等成熟的方案. 但引入消息队列常常也伴随产生如何保证消息不丢失/不重复/有序执行等问题. 我们将在后续内容中深入讨论这些问题.

#### 分布式锁

分布式锁是解决分布式系统共享资源的一种常用手段. 考虑以下问题:

* 如何基于关系型数据库实现分布式锁?
* 如何基于Redis实现分布式锁? 有什么优缺点?
* 如何使用Etcd或ZooKeeper实现分布式锁? 有什么优缺点?
* 如何保证方案实现的可用性?
* 如何避免死锁?
* 如何避免脑裂?

##### 基于关系型数据库实现分布式锁

基于关系型数据库的分布式锁实现是程序员的必备技能, 我在[MySQL锁和事务](https://blog.angelmsger.com/InnoDB锁和事务/)的文章中讨论过MySQL存储引擎InnoDB对于锁逻辑的处理以及这些实现如何阻止脏读, 幻读和不可重复读的产生. 基于关系型数据库的锁实现虽然简单, 但也需要考虑事务的隔离级别和死锁的问题, 比如如下语句在对应行记录上加锁:

```mysql
SELECT id FROM t_orders WHERE order_id = 1 FOR UPDATE;
```

死锁的产生依赖4个必要条件: 互斥, 占有且等待, 不可剥夺和循环等待条件等, 因此解决的思路也在于打破这些条件, 落地到实践中常用的手段有按序取锁, 超时释放等.

上述基于关系型数据库行锁的方案是悲观锁, 事务将持有锁直至提交或回滚. 我们也可以通过乐观锁的方式避免这种情况, 如下:

```mysql
// 获取版本
SELECT id, amount, version AS _version FROM t_orders WHERE order_id = 1;
// ... 执行计算
// 提交更新
UPDATE t_orders SET amount = 65535,  version = _version + 1 WHERE order_id = 1 AND version = _version;
```

若更新语句返回记录数为0, 则说明事务执行期间有其他事务修改了对应记录, 此时需要业务代码进行异常处理.

##### 基于分布式缓存实现分布式锁

基于分布式缓存实现分布式锁是也是常见的方案, 其能在流量高峰期避免大量请求访问数据库, 提升系统的响应能力. 基于Redis实现分布式锁的在数据一致性方面的风险我们在前文已有提及, 此处我们依然假设通过该方案实现分布式锁. 加锁方案如下:

```redis
SET lock_key unique_value NX PX 10000
```

其中:

* lock_key: 加锁目标.
* unique_value: 客户端唯一标识, 用于判定锁资源持有方.
* NX: 仅在锁不存在时置锁.
* PX: 设置超时时间, 防止客户端出现异常时锁资源无法释放.

解锁即将该键值从Redis中删除, 但需要注意, **只有当前线程持有对应锁资源时才能释放该锁**, 这一点在实现过程中很容易被忽略, 就有可能出现下图中的情况:

![分布式锁的错误实现](/images/%E6%9E%B6%E6%9E%84%E5%B8%B8%E8%AF%86%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81%E7%9A%84%E9%94%99%E8%AF%AF%E5%AE%9E%E7%8E%B0-6125117.png)

由于Redis对Lua脚本的执行时原子性的, 因此可以使用下述代码实现解锁过程:

```lua
if redis.call('GET', KEYS[1]) === ARGV[1] then
  return redis.call('DEL', KEYS[1])
else
  return 0
end
```

还有一个问题, 怎样为锁确定一个合理的超时时间呢? 如果设置的过长, 会在异常时由于锁长时间不能释放而影响性能, 如果设置的过短, 则可能任务还在正常执行完成锁已被自动释放. 这一问题的一个解决思路是由另一个守护线程定期检查任务的执行状态并对锁续期.

由于Redis的设计遵循AP模型, 正如前文提及, 直接使用上述方案时脑裂的情况是不可避免的.  Redis作者给出的[RedLock](https://redis.io/topics/distlock)方案的基本思路是向多个Redis实例依次请求取锁, 获得半数以上成功时才视为加锁成功. 相比之下, Etcd使用的Raft协议和ZooKeeper使用的Zab协议都是强一致性共识算法, 更加适合分布式锁场景.

### 运算

# RPC

在目前的趋势下, 应用系统发展到一定规模后都会向着服务化的方向演进, 单体系统转变为由若干服务组成的系统, 服务间通过远程调用(RPC, Remote procedure call)进行通信. 作为一项基础设施, 其特性, 设计和性能也将影响系统的方方面面.

RPC的技术方案有很多, 从简单的发起Http调用, 到成熟的RPC框架如[Dubbo](https://dubbo.apache.org/zh/), [Thrift](https://thrift.apache.org/)和[gRPC](https://grpc.io/)等, 怎样选择呢. 假设在如下的典型场景中:

![RPC](/images/%E6%9E%B6%E6%9E%84%E5%B8%B8%E8%AF%86%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/RPC.png)

* 如何评估某RPC框架是否满足上述场景的性能需要?
* 如何设定特定节点的超时时间?

如上图, 业务网关入网请求并发规模为10k qps, 并依赖上游4个服务. 因此对业务网关与其上有服务通信所使用的的RPC框架而言, 至少需要有能力承载40k qps, 但还需考虑服务间调用的串并行方式和深层依赖关系. 超时策略最简单的方式是对调用链分支请求时间求和并取最大值, 但这种方式忽视了现代RPC框架内部提供的重传, 上游服务降级对链路调用时长的影响等. 不合理的超时策略会扩大服务产生波动时对系统造成的影响, 比如上游服务仍在重传, 下游服务已经由于超时而返回失败, 或上游服务触发降级, 仍能正常提供服务但响应时间发生变化, 但下游服务仍按原响应时间设置超时策略, 导致用户最终仍然无法正常使用等.

RPC框架的核心即**序列化**和**通信**方式. 序列化即如何对数 据进行封包解包, 常见的选择如JSON, XML和[Protobuf](https://developers.google.com/protocol-buffers), 通常是在可读性, 兼容性, 性能和资源消耗间做权衡. 在业务规模较小, 网络吞吐量要求不高的场景中JSON是常见的选择, 发展到一定程度产生相关需求后可以考虑转为Protobuf, 后者在各项指标上都表现良好. 通信是一个老生常谈的问题, 网络编程中的典型I/O模型如下:

* 同步阻塞(BIO)
* 同步非阻塞(NIO)
* 异步非阻塞(AIO)

区分上述 目前我们常用的是BIO和NIO. BIO是

# 消息中间件

# 数据库

# 分布式缓存

# 高性能高可用