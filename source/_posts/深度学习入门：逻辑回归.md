---
title: 深度学习入门：逻辑回归
date: 2023-11-19 10:38:35
categories:
  - AI
tags:
  - AI
  - Deep Learning
mathjax: true
thumbnail: "/images/banner/深度学习入门：逻辑回归.jpg"
typora-root-url: ../../source/
---

# 概述

近一两年由于 [GitHub Copilot](https://github.com/features/copilot), [ChatGPT](https://chat.openai.com/) 的发布, 大语言模型 LLM 逐渐进入普通开发者和大众的视野并受到追捧, 并确实在一些场景中带来了前所未有的功能变革.

我虽然在本科期间对 AI 相关理论有过肤浅的学习, 但坦诚的说几年过去已经非常生疏了. 本着知其然也应知其所以然的原则, 我最近又把一些基础理论捡起来复习了一遍, 并查阅了一些 LLM 原理的相关资料, 在此作为整理.

由于内容比较多, 因此会拆分为**深度学习/神经网络基础**, **图像/自然语言处理**和**生成式 AI** 等若干博文. 主要参考资料是 Andrew NG 老师在 Coursera 上的[课程](https://www.coursera.org/specializations/deep-learning)和李宏毅老师的[公开课](https://www.youtube.com/@HungyiLeeNTU). 深度学习相关的工具链发展很快, 原课程中的一些课后练习包含的代码已经无法在新版本框架中运行, 我会给出修正并提供在 [GitHub](https://github.com/AngelMsger/One/tree/master/note/deeplearningai) 中, 该 Repo 还包含了我整理的课件 PPT.

系列文章以个人知识梳理为主要目的, 一些细节不会展开论述, 没有相关背景知识的同学不建议阅读.

# 神经网络

**神经网络**(Neural Networks, **NN**)是一种模型, 而模型本质上是一个**函数**, 根据输入给出某种意义上的预测结果. 下表列举了这类函数目前的一些应用场景, 涵盖了**机器视觉**(Computer Vision, **CV**), **自然语言处理**(Nature Language Processing, **NLP**)和最近大火的**生成式 AI**(Generative AI)等领域.

| 输入                       | 输出                       | 应用场景 |
| -------------------------- | -------------------------- | -------- |
| 商品特征特征               | 商品价格                   | 价值预测 |
| 广告, 用户画像             | 是否会点击                 | 在线广告 |
| 图像                       | 预定义标签                 | 图片分类 |
| 声音                       | 文本                       | 语音识别 |
| 英文                       | 中文                       | 机器翻译 |
| 图像或雷达数据             | 附近车辆的相对位置         | 自动驾驶 |
| 用户提示词, 已经生成的内容 | 下一个字或像素点的概率分布 | 内容生成 |

模型训练的过程就是根据一定量的输入输出样本**拟合**最可能的函数, 类似我们中学数学中学习的线性回归.

基于神经网络构建模型是现在最为流行的方案, 因为实践已经证明, 它相较于 AI 领域其他传统算法而言, 可以通过**数据量**和**模型规模**的扩大而在复杂问题中表现出良好的性能(在模型训练领域, 性能不止包含计算性能, 也包含准确率).

![数据集规模与性能趋势](/images/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%EF%BC%9A%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/%E6%95%B0%E6%8D%AE%E9%9B%86%E8%A7%84%E6%A8%A1%E4%B8%8E%E6%80%A7%E8%83%BD%E8%B6%8B%E5%8A%BF.png)

神经网络这个名字源自其计算图节点和拓扑结构与生物学中的神经元及其信号传递非常相似. 如下图是一个结构非常简单的, 用于判断图片中是否包含猫猫的神经网络, 本文也将就此问题展开讨论神经网络的实现原理:

![LogReg_kiank](/images/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%EF%BC%9A%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/LogReg_kiank.png)

# 逻辑回归

我们从通过**逻辑回归**(Logistics Regression)实现一个**二元分类**问题开始: 如上图, 根据图片判断是否包含猫猫.

逻辑回归的输入需要是一个**特征向量**(Feature Vector), 这里可以简单理解为一个数字数组, 每一个位置描述了样本的某种特征.

计算机中保存的图片可以理解为多通道像素矩阵, 我们直接将这些通道的像素**拉平**(Flat)**拼接**(Concat)之后就可以得到一个特征向量. 这种处理方式比较直接, 但也满足这个问题的需要. 事实上如何将原始问题场景中的输入转化为保留其在真实世界中特征的向量有时也是一门独立的学问, 比如 NLP 领域的 [word2vec](https://www.tensorflow.org/text/tutorials/word2vec).

![img](/images/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%EF%BC%9A%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/image2vector.png)

我们的模型训练思路大致是这样:

1. 首先我们需要一个标注好的**数据集**. 数据集的内容是一组图片和这些图片对应的标签, 有些图片是猫并且标注为 `true`, 有些图片不是猫并标注为 `false`.
2. 按照一定的比例, 把数据集划分为**训练集**和**测试集**. 我们通过训练集来优化模型, 并且在上验证模型输出的效果.
3. 设计一个**模型**, 包含一组**参数**, 通过模型及其参数和训练集中作为输入的**图片**, 得到**预测值**, 此时准确率很低.
4. 我们通过计算预测值和实际值(样本标签)之间的**差异**, **微调**模型中的参数, 并进入**下一轮迭代**. 此时准确率应当逐渐提升.
5. 经过**一定次数的迭代**, 我们模型的参数会逐渐**收敛**, 即为本次训练的结果.

## 二元分类

我们首先约定一些符号:

- 我们的输入是上面这样一个用来表示图片的向量, 我们记为 $x$.
- $x$ 向量中的第 $i$ 个元素, 是图片中某个像素点在某个通道上的值, 我们记为 $x_{i}$.
- 对于任意输入图片 $x$, 其真实标签(是否为猫)记为 $y$, 我们的预测值记为 $\hat{y}$.

假设我们认为图片中每个像素点在不同通道上的值都是该图片的**特征**, 这些特征都应在预测过程中具备一定**权重**, 那么我们的预测方式可以写作: $w_{1}*x_{1}+w_{2}*x_{2}+\dots+w_{n}*x_{n}+b$.

因为我们解决的是一个二元分类的问题, 所以输出应该是范围为 $[0,1]$ 的浮点数, 含义是结果为 `true`, 或者说输入图片是猫的概率. 因此我们需要在上面的结果上套一个**激活函数**(Activate Function), 它有两个作用:

- 把结果映射到要求的范围内, 激活前结果越大, 激活后应该越接近 1, 激活前结果越小, 激活后应该越接近 0.
- 让结果的计算不再是完全线性的.

这里我们选择 [sigmoid](https://en.wikipedia.org/wiki/Sigmoid_function)($\sigma$) 函数, 它的公式和图示如下, 你可以简单验证它符合上述条件:

![sigmoid](/images/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%EF%BC%9A%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/sigmoid.png)

## 损失函数

### 单个样本

为了评估预测值准确与否, 我们需要得到他和真实值的偏差, 或称为**损失**(Loss), 我们将其记为 $\mathcal{L}(\hat{y}^{(i)}, y^{(i)})$. 偏差的计算当然有很多方法可选, 比如简单直接的差值平方:

$$\mathcal{L}(\hat{y}, y) = \tfrac{1}{2} (\hat{y} - y)^2$$

这个算法最大的问题是, 它和我们后续使用的优化方法 - 梯度下降是冲突的, 我们会在优化的章节里解释. 这里我们直接给出最终使用的算法:

$$\mathcal{L}(\hat{y}^{(i)}, y^{(i)}) =  - y^{(i)}  \log(\hat{y}^{(i)}) - (1-y^{(i)} )  \log(1-\hat{y}^{(i)})$$

你可以通过枚举当 $y = 0$ 和 $y = 1$ 时 $\hat{y}$ 的不同趋势对 $\mathcal{L}(\hat{y}^{(i)}, y^{(i)})$ 值的影响来验证它和前者具有相似的性质.

### 所有样本

单个样本的结果并不能提现出模型在整个训练集上的准确率. 为了表述整体的准确率, 我们再约定一些符号:

- 我们用小括号**上标**来表示训练集中的第 $n$ 个样本, 比如 $x^{(i)}$ 为第 i 个样本.
- 样本和特征可以组合表示, 如 $x^{(i)}_{j}$ 为第 $i$ 个样本的第 $j$ 个特征.

参数在整个训练集上的损失为其在每一样本损失的算数平均值, 称为**成本**(Cost), 计算方式如下:

$$J = \frac{1}{m} \sum_{i=1}^m \mathcal{L}(\hat{y}^{(i)}, y^{(i)})$$

## 梯度下降

**成本**描述的是模型输出与实际标签之间的差异, 为了提升模型的准确率, 我们希望成本应该尽可能小, 换句话说我们希望找到成本函数的**全局最小值**.

我们在大学课本里学过**梯度**(Derivative)的概念, **沿梯度方向的导数最大**, 并且梯度的值就是**方向导数**的值. 导数在图像上体现为**仰角**的大小, 陡峭的地方导数的值更大, 平缓的地方导数的值更小. 如果我们在**凸函数**上任取一点作为起点不断移动, 每次移动以该点的**梯度反方向为方向**, **方向导数的值为步长**, 那么我们将不断向全局最小值的位置移动, 并且在陡峭的位置移动更快, 在接近目标时逐渐变慢, 最终收敛到我们的目标 - 全局最小值, 这就是**梯度下降**(Gradient Descent)算法的核心原理. 我们用一幅降维之后的图举个栗子, 我们从红点出发, 沿着梯度方向逐渐移动, 就走到了全局最小值的位置:

![img](/images/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%EF%BC%9A%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/gradient_descent_1.png)

梯度下降算法生效的必要条件是, 目标函数是一个**凸函数**, 否则梯度下降可能会收敛到一些**局部极小值**而不是全局最小值上, 比如下图中右上角的那个:

![2. 简道云 > 入门 1: 逻辑回归 - Vista > gradient_descent_2.gif](/images/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%EF%BC%9A%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/gradient_descent_2.gif)

这就是我们在选择损失函数时没有使用差值平方来评估差异的原因.

### 正向传播

有了这样的思路之后, 我们尝试优化成本函数的参数. 我们先来约定一些数学和代码符号:

- 为了简化描述, 我们建设输入特征和权重参数的个数为 $2$.
- 用 `w1`, `w2`, `x1`, `x2` 和 `b` 来表示参数 $w_{1}$, $w_{2}$, $x_{1}$, $x_2$ 和 $b$.
- 用 `a` 表示 $\hat{y}$, `a` 是激活值的缩写, 这样命名也是为了和之后扩展为多层神经网时的命名一致.
- 用 `z` 表示 `sigmoid` 函数的输入, 也就是线性计算的值记为中间变量, 即:
  - `z = w1 * x1 + w2 * x2 +b`.
  - `a = sigmoid(z)`.
- 用 `cost` 或 `J` 来表示成本函数的结果.

根据前文的介绍, `a` 即是我们正向预测的结果, 这一过程称为**正向传播**(Forward Propagation). 在第一次正向传播前, 参数会被随机初始化(不能初始化为 0, 见反向传播).

### 反向传播

正向传播很容易理解, 但其参数是随机初始化的, 因此没有准确率可言, 接下来我们将通过**反向传播**(Backward Propagation)来更新这些参数, 从而完成梯度下降. 这一过程也成为**优化**(Optimiazation). 同样的, 我们先来约定一些数学和代码符号:

- 用 `dw1` 来表示数学概念 $\frac{\partial J}{\partial w_{1}}$, 因为训练过程中我们的优化目标不变, 所以在变量名中可以省略. 因此, `db` 就代表 $\frac{\partial J}{\partial b}$.
- 根据导数公式, 易知 $\frac{\mathrm{d} L}{\mathrm{d} a}=-\frac{y}{a}+\frac{1-y}{1-a}$.
- 根据导数公式和链式规则, 易知 $\frac{\mathrm{d} L}{\mathrm{d} z}=\frac{\mathrm{d} L}{\mathrm{d} a}\frac{\mathrm{d} a}{\mathrm{d} z}=a-y$.
- 根据导数公式和链式规则, 易知 $\frac{\mathrm{d} L}{\mathrm{d} w_{i}}=\frac{\mathrm{d} L}{\mathrm{d} z}\frac{\mathrm{d} z}{\mathrm{d} w_{i}}=(a-y)*x_{i}$.
- 根据导数公式和链式规则, 易知 $\frac{\mathrm{d} L}{\mathrm{d} b}=a-y$.
- 成本 `J` 是对损失 `L` 的算数平均值, 因此对求导过程没有影响. 对方向导数的值直接做算数平均处理.

在这样的符号约定和背景知识下, 我们来实现梯度下降算法的一次迭代的伪代码:

![迭代伪代码](/images/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%EF%BC%9A%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/%E8%BF%AD%E4%BB%A3%E4%BC%AA%E4%BB%A3%E7%A0%81.png)

根据梯度下降的原理, 在求得梯度后我们将其作用于参数, 从而使成本函数的结果向最小值收敛, 通常我们还会引入参数**学习率**(Learning Rate, $\alpha$)来控制单次迭代对参数的影响.

```python
w1 -= learning_rate * dw1
w2 -= learning_rate * dw2
b -= learning_rate * db
```

当我们迭代一定次数后, 模型的参数将会收敛.

## 向量化

在梯度下降算法中, 我们每次迭代涉及很多个样本(比如一个训练集可能有上万张图片或者用户), 每个样本又有很多个特征(像素点或者用户画像维度), 我们需要计算每个样本对每个特征对应权重的微分, 如果在代码中写 for 循环实现性能会很差, 所以我们需要引入**向量化**(Vectorization).

在计算机术语中, 向量化指的是利用利用底层硬件能力将批处理过程从一次处理一个转化为一次处理多个. 我们引入向量化的方式是将通过循环实现的运算转化为**矩阵运算**, 很多矩阵运算可以在硬件层面(比如 GPU)进行并行化处理, 速度会快很多(有几百倍的差距).

为了将输入和参数转化为矩阵, 我们需要约定一些新的符号, 总体是用大写字母表示由此前对应小写字母关联向量构成的矩阵:

- 用大写 $W$ 表示由权重参数 $w$ 行向量纵向堆积后的矩阵, 在逻辑回归中, 由于我们只有一组权重参数, 因此矩阵形状为 $1*n$.
- 用大写 $X$ 表示由输入 $x^{(i)}$ 列向量横向堆积后的矩阵, 由于我们有 $m$ 个样本, 每个样本形状为 $n * 1$, 因此矩阵形状为 $n * m$.
- 由于 $b$ 此前是一个常数标量, 向量化后也只会扩展为行向量, 因此一般不用大写表示. 另外由于我们这里只讨论一个单元的逻辑回归, 因此 $b$ 的实际形状仍为 $1*1$.
- 用大写 $Z$ 表示由线性结果列向量 $z^{(i)}$ 横向堆积后的矩阵. 在逻辑回归中, $z^{(i)}$ 的形状为 $1*1$, 因此 $Z$ 的形状为 1 \* m.
- 用大写 $A$ 表示应用激活函数后的预测结果, 其形状与 $Z$ 相同, 为 $1 * m$.
- 用大写 $Y$ 表示输入的实际结果, 其形状与 $A$ 相同, 为 $1 * m$.
- 当我们从逻辑回归转向神经网络时, 通常这样的结构不止一层, 并且每层也不止一个单元, 此时我们会引入右上角方括号标识来标记变量关联的层级, 如 $W^{[1]}$ 表示第一层的权重.

![向量化表示](/images/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%EF%BC%9A%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/%E5%90%91%E9%87%8F%E5%8C%96%E8%A1%A8%E7%A4%BA.png)

根据矩阵运算规则:

- $Z = WX + b$, $Z$ 矩阵中每个元素的值分别对应一个样本的 $z^{(i)}$.
- $A = sigmoid (Z)$, $A$ 矩阵中每个元素的值分别对应一个样本的 $a^{(i)}$(即 $\hat{y^{(i)}}$).
- $dZ = A - Y$, $dZ$ 矩阵中每个元素的值分别对应一个样本的 $dz^{(i)}$.
- $dW = \frac{1}{m}XdZ^T$, $dW$ 矩阵中每个元素的值分别对应一个参数的梯度.
- $db = \frac{1}{m} sum(dZ)$, 是一个标量.

每轮迭代参数更新逻辑如下:

```python
W -= learning_rate * dW
b -= learning_rate * db
```

由于每轮迭代依赖前一轮迭代的结果, 因此迭代之间并不能向量化, 但我们后续还会介绍其他充分发挥硬件能力并提升性能的手段.

# 代码实现

由于篇幅所限, 这里会删减非核心逻辑, 完整代码见 GitHub 中的 Notebook.

```python
# 导入包.
import numpy as np
from scipy import ndimage
from lr_utils import load_dataset

# 加载数据集/测试集.
train_set_x_orig, train_set_y, test_set_x_orig, test_set_y, classes = load_dataset()

# 定义激活函数.
def sigmoid(z):
    return (np.exp(-z) + 1) ** -1

# 传播.
def propagate(w, b, X, Y):
  m = X.shape[1]
  # 正向传播.
  A = sigmoid(np.dot(w.T, X) + b)
  cost = np.sum(Y * np.log(A) + (1 - Y) * np.log(1 - A)) / -m
  # 反向传播.
  dz = A - Y
  dw = np.dot(X, dz.T) / m
  db = np.sum(dz) / m
  cost = np.squeeze(cost)
  # 返回梯度和本次成本.
  grads = {
    "dw": dw,
    "db": db
  }
  return grads, cost

# 优化.
def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):
  costs = []
  for i in range(num_iterations):
    # 一次迭代, 传播.
    grads, cost = propagate(w, b, X, Y)
    dw = grads["dw"]
    db = grads["db"]
    # 通过梯度更新参数.
    w = w - learning_rate * dw
    b = b - learning_rate * db
    # 记录成本变化, 必要的打印.
    if i % 100 == 0:
      costs.append(cost)
    if print_cost and i % 100 == 0:
      print ("Cost after iteration %i: %f" %(i, cost))
  params = {
    "w": w,
    "b": b
  }
  grads = {
    "dw": dw,
    "db": db
  }
  return params, grads, costs

# 预测函数(相当于一次正向过程).
def predict(w, b, X):
  m = X.shape[1]
  Y_prediction = np.zeros((1, m))
  w = w.reshape(X.shape[0], 1)
  A = sigmoid(np.dot(w.T, X) + b)
  # 将分布转化为预测.
  for i in range(A.shape[1]):
    Y_prediction[0, i] = 0 if A[0, i] <= 0.5 else 1
return Y_prediction

# 合并模型.
def model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.5, print_cost = False):
  # 初始化参数.
  w, b = initialize_with_zeros(X_train.shape[0])
  # 梯度下降.
  parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost)
  w = parameters['w']
  b = parameters['b']
  # 训练集/测试集上的预测结果.
  Y_prediction_test = predict(w, b, X_test)
  Y_prediction_train = predict(w, b, X_train)
  d = {
    "costs": costs,
    "Y_prediction_test": Y_prediction_test,
    "Y_prediction_train" : Y_prediction_train,
    "w" : w,
    "b" : b,
    "learning_rate" : learning_rate,
    "num_iterations": num_iterations
  }
  return d
```

这份朴素实现能够在目标训练/测试集跑出以下结果:

```shell
train accuracy: 99.04306220095694 %
test accuracy: 70.0 %
```

尽管结果看似还行, 但如果拿一些不那么标准的图去验证, 会发现还是很容易预测出错的, 比如我的头像:

![震惊猫猫头](/images/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%EF%BC%9A%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/%E9%9C%87%E6%83%8A%E7%8C%AB%E7%8C%AB%E5%A4%B4.png)

# 结语

本文简要介绍了神经网络基础的一些理论知识, 并通过手写逻辑回归进行了验证. 深度学习的神奇之处就在于, 当我们把这种思想运用在更大规模和更复杂的网络结构上时会体现出一些神奇的效果, 我们在下一篇文章中讨论.
